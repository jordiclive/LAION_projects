#model_name_or_path: 't5-large'
model_name_or_path: 'google/flan-t5-xxl'
# resume_from_checkpoint: "/admin/home-jordiclive/LAION_projects/FLAN_code/results/20230128_2007/epoch=0-step=1.ckpt"
train_batch_size: 8
eval_batch_size: 1
logger: wandb
warmup_steps: 2000
learning_rate: 5e-4
num_train_epochs: 4
gradient_accumulation_steps: 32
num_workers: 2
adam_epsilon: 1e-8
data_path: '/admin/home-jordiclive/LAION_projects/summarization_data_prep/processing'
wb_name: "retrain-more-prompts-13B"
wb_project: "FLAN_Summarizer"
wb_entity: "jordanclive"
val_check_interval: 0.5
#skip_val: True
#limit_val_batches: None
label_smoothing: 0
max_seq_length: 412
max_target_length: 150
eval_max_gen_length: 150
freeze_embeds: True
num_sanity_val_steps: -1
test_outputs: outputs
eval_min_length: 5
#offline: True
# gpus: 1
visible_devices: "0,1,2,3,4,5,6,7"
gpus: 8
# visible_devices: "0,1,2,3,4,5,6,7"
val_metric: 'val_loss_epoch'
precision: 'bf16'

#debug_mode: True
# save_generations: True

#skip_val: True
#offline: True
#local: True
# load_checkpoint: True
#skip_val: False
